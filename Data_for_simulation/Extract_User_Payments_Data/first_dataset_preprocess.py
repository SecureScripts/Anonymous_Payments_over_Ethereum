#!/usr/bin/env python3
"""
first_dataset_preprocess.py

Initial preprocessing script for the IBM credit card transaction dataset.

This script reads the raw dataset file:

    credit_card_transactions-ibm_v2.csv

(which must be located in the same directory as this script), applies basic
cleaning filters, and produces an output CSV:

    user_transaction_pairs.csv

The output file contains one row per user, with a JSON-encoded list of
(timestamp_ms, amount) pairs:

    User,pairs_json
    U0001,"[[timestamp_ms, amount], [timestamp_ms, amount], ...]"

Filtering rules applied:
- Remove fraudulent transactions (Is Fraud? == "Yes")
- Remove transactions with errors (Errors? not empty)
- Remove negative amounts
- Parse timestamps from Year/Month/Day/Time (Time assumed to be "HH:MM")

The script processes the input CSV in chunks to avoid excessive memory usage.

Notes:
- The original dataset is large (~2GB) and is not included in this repository.
  Please download it and place it in this folder with the exact file name:
  "credit_card_transactions-ibm_v2.csv".
"""

from __future__ import annotations

import json
from collections import defaultdict
from typing import DefaultDict, List, Tuple

import pandas as pd


# -----------------------------------------------------------------------------
# Configuration
# -----------------------------------------------------------------------------

# Raw dataset file (must be in the same folder as this script)
INPUT_CSV = "credit_card_transactions-ibm_v2.csv"

# Output file generated by this script
OUTPUT_CSV = "user_transaction_pairs.csv"

# Read only the columns needed for preprocessing
USECOLS = ["User", "Amount", "Year", "Month", "Day", "Time", "Is Fraud?", "Errors?"]

# Chunk size: adjust depending on available RAM
CHUNK_SIZE = 200_000


# -----------------------------------------------------------------------------
# Helpers
# -----------------------------------------------------------------------------

def parse_amount(series: pd.Series) -> pd.Series:
    """
    Parse amounts formatted like "$44.41" or "$-12.43" and convert to float.

    Parameters
    ----------
    series : pandas.Series
        Amount column as strings.

    Returns
    -------
    pandas.Series
        Parsed numeric amounts as float.
    """
    return (
        series.astype(str)
        .str.replace(r"[\$,]", "", regex=True)
        .astype(float)
    )


# -----------------------------------------------------------------------------
# Main processing
# -----------------------------------------------------------------------------

def main() -> None:
    """
    Main preprocessing routine:
    - reads the dataset in chunks
    - filters out unwanted rows
    - converts date/time into UNIX timestamp (ms)
    - accumulates per-user (timestamp_ms, amount) pairs
    - writes one row per user to OUTPUT_CSV
    """
    # Accumulator: user -> list of (timestamp_ms, amount)
    acc: DefaultDict[str, List[Tuple[int, float]]] = defaultdict(list)

    # Process file in chunks to keep memory usage manageable
    for chunk in pd.read_csv(
        INPUT_CSV,
        usecols=USECOLS,
        chunksize=CHUNK_SIZE,
        dtype={
            "User": "string",
            "Amount": "string",
            "Year": "int32",
            "Month": "int32",
            "Day": "int32",
            "Time": "string",
            "Is Fraud?": "string",
            "Errors?": "string",
        },
    ):
        # ---------------------------------------------------------------------
        # Early filtering (as soon as possible for performance)
        # ---------------------------------------------------------------------

        # Keep only non-fraudulent transactions
        fraud_yes = chunk["Is Fraud?"].fillna("").str.strip().str.lower().eq("yes")

        # Keep only transactions without errors (empty string or NaN)
        errors_empty = chunk["Errors?"].fillna("").astype(str).eq("")

        chunk = chunk[(~fraud_yes) & (errors_empty)]
        if chunk.empty:
            continue

        # ---------------------------------------------------------------------
        # Amount parsing and filtering
        # ---------------------------------------------------------------------

        chunk["Amount_val"] = parse_amount(chunk["Amount"])
        chunk = chunk[chunk["Amount_val"] >= 0]
        if chunk.empty:
            continue

        # ---------------------------------------------------------------------
        # Timestamp construction
        # ---------------------------------------------------------------------

        # Time is expected to be "HH:MM"
        hhmm = chunk["Time"].fillna("00:00").str.split(":", n=1, expand=True)

        # Supports "H:MM" as well, because casting to int handles it
        hours = hhmm[0].astype("int16")
        minutes = hhmm[1].astype("int16")

        # Build datetime from Year/Month/Day
        base_dt = pd.to_datetime(
            {
                "year": chunk["Year"].astype("int32"),
                "month": chunk["Month"].astype("int32"),
                "day": chunk["Day"].astype("int32"),
            },
            errors="coerce",
            utc=False,   # keep as naive datetime
        )

        # Add hours and minutes
        dt = base_dt + pd.to_timedelta(hours, unit="h") + pd.to_timedelta(minutes, unit="m")

        # Drop rows where datetime failed to parse
        valid = dt.notna()
        if not valid.all():
            chunk = chunk[valid]
            dt = dt[valid]

        if chunk.empty:
            continue

        # Convert to UNIX timestamp in milliseconds:
        # datetime64[ns] -> int64 nanoseconds -> milliseconds
        ts_ms = (dt.view("int64") // 1_000_000).astype("int64")

        # ---------------------------------------------------------------------
        # Accumulate (user, timestamp_ms, amount)
        # ---------------------------------------------------------------------

        for user, ts, amt in zip(chunk["User"].astype(str), ts_ms, chunk["Amount_val"]):
            acc[user].append((int(ts), float(amt)))

    # -------------------------------------------------------------------------
    # Final output
    # -------------------------------------------------------------------------

    if acc:
        users: List[str] = []
        json_lists: List[str] = []

        for user, pairs in acc.items():
            # Ensure chronological order
            pairs.sort(key=lambda x: x[0])

            users.append(user)
            json_lists.append(json.dumps(pairs))

        out_df = pd.DataFrame({"User": users, "pairs_json": json_lists})
        out_df = out_df.sort_values("User")  # stable output order

        out_df.to_csv(OUTPUT_CSV, index=False)
        print(f"[INFO] Output file created: {OUTPUT_CSV} (users: {len(out_df)})")
    else:
        # No valid rows after filtering
        pd.DataFrame(columns=["User", "pairs_json"]).to_csv(OUTPUT_CSV, index=False)
        print(f"[WARNING] No valid rows after filtering. Empty file created: {OUTPUT_CSV}")


if __name__ == "__main__":
    main()
